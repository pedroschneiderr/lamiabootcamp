{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":9610593,"sourceType":"datasetVersion","datasetId":5864210}],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#!pip install tensorflow==2.8.0","metadata":{"execution":{"iopub.status.busy":"2024-10-24T13:32:02.470055Z","iopub.execute_input":"2024-10-24T13:32:02.470528Z","iopub.status.idle":"2024-10-24T13:32:02.476711Z","shell.execute_reply.started":"2024-10-24T13:32:02.470486Z","shell.execute_reply":"2024-10-24T13:32:02.475390Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import cv2 \nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom tensorflow.keras.models import load_model","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-10-24T13:32:02.483190Z","iopub.execute_input":"2024-10-24T13:32:02.483568Z","iopub.status.idle":"2024-10-24T13:32:07.571699Z","shell.execute_reply.started":"2024-10-24T13:32:02.483529Z","shell.execute_reply":"2024-10-24T13:32:07.570397Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"model = load_model('/kaggle/input/arquivos-card-20/Material/modelo_02_expressoes.h5')","metadata":{"execution":{"iopub.status.busy":"2024-10-24T13:32:07.573120Z","iopub.execute_input":"2024-10-24T13:32:07.573829Z","iopub.status.idle":"2024-10-24T13:32:08.504457Z","shell.execute_reply.started":"2024-10-24T13:32:07.573787Z","shell.execute_reply":"2024-10-24T13:32:08.503185Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"video_file = '/kaggle/input/arquivos-card-20/Material/Videos/video_teste04.mp4'\ncap = cv2.VideoCapture(video_file) # carrega o vídeo \nconected, video = cap.read() # conected: bool para indicar se o vídeo foi carregado; video: objeto do vídeo\nprint(conected, video.shape)","metadata":{"execution":{"iopub.status.busy":"2024-10-24T14:42:34.166584Z","iopub.execute_input":"2024-10-24T14:42:34.167209Z","iopub.status.idle":"2024-10-24T14:42:34.191711Z","shell.execute_reply.started":"2024-10-24T14:42:34.167161Z","shell.execute_reply":"2024-10-24T14:42:34.189841Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stdout","text":"True (360, 640, 3)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Redimensionando o vídeo para acelerar o treinamento","metadata":{}},{"cell_type":"code","source":"resize = True # enabler do processo\nmax_width = 600 # a altura é definida depois, proporcionalmente\n\nif (resize and video.shape[1] > max_width):\n    proportion = video.shape[1]/video.shape[0] # calcula a proporção das dimensões do video\n    video_width = max_width\n    video_height = int(video_width/proportion) # define a altura de acordo com a nova largura e a proporção original\nelse: # caso o resize não for feito\n    video_width = video.shape[1]\n    video_height = video.shape[0]","metadata":{"execution":{"iopub.status.busy":"2024-10-24T14:42:35.352577Z","iopub.execute_input":"2024-10-24T14:42:35.353183Z","iopub.status.idle":"2024-10-24T14:42:35.361952Z","shell.execute_reply.started":"2024-10-24T14:42:35.353132Z","shell.execute_reply":"2024-10-24T14:42:35.360106Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"markdown","source":"# Definindo as configurações do output","metadata":{}},{"cell_type":"code","source":"file_name = 'resultado_video_teste04.avi'\nfourcc = cv2.VideoWriter_fourcc(*'XVID') # especificação do codec por fourcc\nfps = 24\n\n# criando o objeto de saída final:\nvideo_output = cv2.VideoWriter(file_name, # nome\n                               fourcc, # codec\n                               fps, # frames por segundo\n                               (video_width, video_height)) # dimensões","metadata":{"execution":{"iopub.status.busy":"2024-10-24T14:42:36.999306Z","iopub.execute_input":"2024-10-24T14:42:37.000405Z","iopub.status.idle":"2024-10-24T14:42:37.009135Z","shell.execute_reply.started":"2024-10-24T14:42:37.000351Z","shell.execute_reply":"2024-10-24T14:42:37.007268Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"markdown","source":"# Processamento e gravação do resultado","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.preprocessing.image import img_to_array\nimport time\n\n# caminho para o modelo de detecção de faces:\nhaarcascade_faces = '/kaggle/input/arquivos-card-20/Material/haarcascade_frontalface_default.xml'\nsmall_font, big_font = 0.4, 0.7\nfont = cv2.FONT_HERSHEY_SIMPLEX\nemotions = ['Anger', 'Disgust', 'Fear', 'Happiness', 'Sadness', 'Surprise', 'Neutral']\n\nwhile (1): # vai rodar até o fim do vídeo\n    conected, frame = cap.read() # conected: indica se o frame foi lido; frame: o objeto de frame\n    \n    if not conected: # se o frame não foi capturado, encerra o processamento\n        print('Finishing')\n        break\n    \n    t = time.time() # registro de tempo do início da iteração\n    \n    if resize:\n        frame = cv2.resize(frame, (video_width, video_height))\n    \n    face_cascade = cv2.CascadeClassifier(haarcascade_faces) # carrega o modelo para detecção de ROIs\n    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY) # converte de BGR para escala de cinza, o que deixa a imagem 3x mais leve\n    \n    # aplica o modelo de detecção de faces:\n    faces = face_cascade.detectMultiScale(gray, scaleFactor=1.2, minNeighbors=5, minSize=(30,30))\n    \n    if len(faces) > 0: # se alguma face foi detectada\n        for (x, y, w, h) in faces: # coodernadas da face\n            frame = cv2.rectangle(frame, (x,y), (x+w, y+h+10), (255,50,50), 2) # desenha um retângulo em torno da face\n            roi = gray[y: y+h, x: x+w] # extrai a ROI\n            roi = cv2.resize(roi, (48, 48)) # faz o redimensionamento da ROI de acordo com o modelo\n            roi = roi.astype('float')/255.0 # converte para float e normaliza\n            roi = img_to_array(roi) # converte para array e adiciona a dimensão unitário de canal de cor\n            roi = np.expand_dims(roi, axis = 0) # adiciona a dimensão de batch\n            \n            preds = model.predict(roi)[0] # faz a predição e retorna as probabilidades para cada emoção\n            \n            #if preds is not None: # se houve de fato um resultado retornado\n            label = np.argmax(preds) # pega o índice (que coincide com a classe) da maior prob\n            cv2.putText(frame, emotions[label], (x, y - 10), font, big_font, (255, 255, 255), 1, cv2.LINE_AA) # adiciona o texto, em branco, à face com o nome da emoção de detectada\n    \n    # adiciona um texto indicando o tempo de processamento da(s) face(s) no frame:\n    cv2.putText(frame, f'frame processed in {time.time() - t:.2f} seconds',\n                (20, video_height-20), font, small_font, (255, 255, 255),\n                1, cv2.LINE_AA)\n    \n    video_output.write(frame) # grava o frame no arquivo de saída\nprint(\"Finished\")\nvideo_output.release() # cria o arquivo do video e salva no diretório de trabalho","metadata":{"execution":{"iopub.status.busy":"2024-10-24T14:42:38.529708Z","iopub.execute_input":"2024-10-24T14:42:38.530336Z","iopub.status.idle":"2024-10-24T14:43:45.259907Z","shell.execute_reply.started":"2024-10-24T14:42:38.530288Z","shell.execute_reply":"2024-10-24T14:43:45.258427Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stdout","text":"Finishing\nFinished\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}